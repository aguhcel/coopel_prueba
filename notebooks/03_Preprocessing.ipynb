{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7aa867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from itables import init_notebook_mode\n",
    "init_notebook_mode(all_interactive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700b43d",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Carga los datos del archivo CSV con manejo de encoding\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Ruta al archivo CSV\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con los datos cargados\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        print(f\"Datos cargados exitosamente con encoding UTF-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "        print(f\"Datos cargados exitosamente con encoding ISO-8859-1\")\n",
    "    \n",
    "    print(f\"Forma del dataset: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    \"\"\"\n",
    "    Elimina registros duplicados del dataset\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame original\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame sin duplicados\n",
    "    \"\"\"\n",
    "    initial_shape = df.shape[0]\n",
    "    df_clean = df.drop_duplicates()\n",
    "    final_shape = df_clean.shape[0]\n",
    "    \n",
    "    print(f\"Registros duplicados eliminados: {initial_shape - final_shape}\")\n",
    "    print(f\"Registros restantes: {final_shape}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def handle_null_customer_id(df):\n",
    "    \"\"\"\n",
    "    Elimina registros con CustomerID nulo\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame original\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame sin CustomerID nulos\n",
    "    \"\"\"\n",
    "    initial_shape = df.shape[0]\n",
    "    null_customers = df['CustomerID'].isnull().sum()\n",
    "    \n",
    "    print(f\"Registros con CustomerID nulo: {null_customers}\")\n",
    "    \n",
    "    if null_customers > 0:\n",
    "        # Análisis por país antes de eliminar\n",
    "        country_nulls = df.groupby('Country').agg({\n",
    "            'CustomerID': lambda x: x.isnull().sum()\n",
    "        }).query('CustomerID > 0').sort_values('CustomerID', ascending=False)\n",
    "        \n",
    "        print(\"Países con CustomerID nulos:\")\n",
    "        for country, nulls in country_nulls['CustomerID'].items():\n",
    "            total = len(df[df['Country'] == country])\n",
    "            percent = (nulls / total) * 100\n",
    "            print(f\"  {country}: {nulls} nulos de {total} transacciones ({percent:.1f}%)\")\n",
    "    \n",
    "    df_clean = df[df['CustomerID'].notna()].copy()\n",
    "    final_shape = df_clean.shape[0]\n",
    "    \n",
    "    print(f\"Registros eliminados: {initial_shape - final_shape}\")\n",
    "    print(f\"Registros restantes: {final_shape}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def filter_cancelled_transactions(df):\n",
    "    \"\"\"\n",
    "    Filtra transacciones de cancelación (InvoiceNo que empiezan con 'C')\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame original\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame sin transacciones canceladas\n",
    "    \"\"\"\n",
    "    initial_shape = df.shape[0]\n",
    "    cancelled_mask = df['InvoiceNo'].str.startswith('C', na=False)\n",
    "    cancelled_count = cancelled_mask.sum()\n",
    "    \n",
    "    print(f\"Transacciones de cancelación encontradas: {cancelled_count}\")\n",
    "    \n",
    "    if cancelled_count > 0:\n",
    "        # Análisis de las cancelaciones\n",
    "        print(\"Análisis de transacciones canceladas:\")\n",
    "        cancelled_df = df[cancelled_mask]\n",
    "        print(f\"  Clientes únicos con cancelaciones: {cancelled_df['CustomerID'].nunique()}\")\n",
    "        print(f\"  Países con cancelaciones: {cancelled_df['Country'].nunique()}\")\n",
    "        print(f\"  Total quantity en cancelaciones: {cancelled_df['Quantity'].sum()}\")\n",
    "    \n",
    "    df_clean = df[~cancelled_mask].copy()\n",
    "    final_shape = df_clean.shape[0]\n",
    "    \n",
    "    print(f\"Registros eliminados: {initial_shape - final_shape}\")\n",
    "    print(f\"Registros restantes: {final_shape}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def handle_inconsistent_data(df):\n",
    "    \"\"\"\n",
    "    Maneja datos inconsistentes: Quantity negativa y UnitPrice = 0\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame original\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con datos consistentes\n",
    "    \"\"\"\n",
    "    initial_shape = df.shape[0]\n",
    "    \n",
    "    # Analizar quantity negativa\n",
    "    negative_quantity = (df['Quantity'] < 0).sum()\n",
    "    zero_quantity = (df['Quantity'] == 0).sum()\n",
    "    \n",
    "    # Analizar UnitPrice = 0 o negativo\n",
    "    zero_price = (df['UnitPrice'] == 0).sum()\n",
    "    negative_price = (df['UnitPrice'] < 0).sum()\n",
    "    \n",
    "    print(f\"Análisis de datos inconsistentes:\")\n",
    "    print(f\"  Quantity negativa: {negative_quantity}\")\n",
    "    print(f\"  Quantity igual a 0: {zero_quantity}\")\n",
    "    print(f\"  UnitPrice igual a 0: {zero_price}\")\n",
    "    print(f\"  UnitPrice negativo: {negative_price}\")\n",
    "    \n",
    "    # Filtrar datos inconsistentes\n",
    "    consistent_mask = (\n",
    "        (df['Quantity'] > 0) &\n",
    "        (df['UnitPrice'] > 0)\n",
    "    )\n",
    "    \n",
    "    df_clean = df[consistent_mask].copy()\n",
    "    final_shape = df_clean.shape[0]\n",
    "    \n",
    "    print(f\"Registros eliminados: {initial_shape - final_shape}\")\n",
    "    print(f\"Registros restantes: {final_shape}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def balance_geographical_data(df):\n",
    "    \"\"\"\n",
    "    Balancea los datos geográficos eliminando países con pocas transacciones\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame original\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame balanceado geográficamente\n",
    "    \"\"\"\n",
    "    # Clasificar países por volumen de transacciones\n",
    "    country_stats = df['Country'].value_counts()\n",
    "    \n",
    "    # Definir tiers\n",
    "    tier1 = country_stats[country_stats >= 1000].index.tolist()\n",
    "    tier2 = country_stats[(country_stats >= 100) & (country_stats < 1000)].index.tolist()\n",
    "    tier3 = country_stats[country_stats < 100].index.tolist()\n",
    "    \n",
    "    print(f\"Clasificación por tiers:\")\n",
    "    print(f\"  Tier 1 (>=1000 trans): {len(tier1)} países - {country_stats[tier1].sum():,} transacciones\")\n",
    "    print(f\"  Tier 2 (100-999 trans): {len(tier2)} países - {country_stats[tier2].sum():,} transacciones\")\n",
    "    print(f\"  Tier 3 (<100 trans): {len(tier3)} países - {country_stats[tier3].sum():,} transacciones\")\n",
    "    \n",
    "    # Asignar tiers\n",
    "    def assign_tier(country):\n",
    "        if country in tier1:\n",
    "            return 'Tier1_Principal'\n",
    "        elif country in tier2:\n",
    "            return 'Tier2_Mediano'\n",
    "        else:\n",
    "            return 'Tier3_Pequeno'\n",
    "    \n",
    "    df['Country_Tier'] = df['Country'].apply(assign_tier)\n",
    "    \n",
    "    # Asignar regiones\n",
    "    def assign_region(country):\n",
    "        europe = ['United Kingdom', 'Germany', 'France', 'Spain', 'Netherlands', \n",
    "                  'Belgium', 'Switzerland', 'Austria', 'Italy', 'Portugal', 'Norway',\n",
    "                  'Denmark', 'Finland', 'Sweden', 'Poland', 'Cyprus']\n",
    "        \n",
    "        asia_pacific = ['Australia', 'Japan', 'Singapore', 'Hong Kong']\n",
    "        americas = ['USA', 'Canada', 'Brazil']\n",
    "        \n",
    "        if country in europe:\n",
    "            return 'Europa'\n",
    "        elif country in asia_pacific:\n",
    "            return 'Asia_Pacifico'\n",
    "        elif country in americas:\n",
    "            return 'Americas'\n",
    "        else:\n",
    "            return 'Otros'\n",
    "    \n",
    "    df['Region'] = df['Country'].apply(assign_region)\n",
    "    \n",
    "    # Filtrar para mantener balance\n",
    "    initial_shape = df.shape[0]\n",
    "    df_balanced = df[\n",
    "        (df['Country_Tier'].isin(['Tier1_Principal', 'Tier2_Mediano'])) &\n",
    "        (df['Region'].isin(['Europa', 'Asia_Pacifico', 'Americas']))\n",
    "    ].copy()\n",
    "    \n",
    "    final_shape = df_balanced.shape[0]\n",
    "    \n",
    "    print(f\"Filtrado geográfico aplicado:\")\n",
    "    print(f\"  Registros eliminados: {initial_shape - final_shape}\")\n",
    "    print(f\"  Registros restantes: {final_shape}\")\n",
    "    print(f\"  Porcentaje mantenido: {(final_shape / initial_shape) * 100:.1f}%\")\n",
    "    \n",
    "    return df_balanced\n",
    "\n",
    "def handle_skewed_data(df):\n",
    "    \"\"\"\n",
    "    Maneja datos sesgados aplicando transformaciones\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame original\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con transformaciones aplicadas\n",
    "    \"\"\"\n",
    "    print(\"Análisis de sesgo en UnitPrice:\")\n",
    "    \n",
    "    # Estadísticas originales\n",
    "    original_skew = stats.skew(df['UnitPrice'])\n",
    "    original_kurtosis = stats.kurtosis(df['UnitPrice'])\n",
    "    \n",
    "    print(f\"  Sesgo original: {original_skew:.3f}\")\n",
    "    print(f\"  Curtosis original: {original_kurtosis:.3f}\")\n",
    "    \n",
    "    # Transformación logarítmica\n",
    "    df['UnitPrice_Log'] = np.log1p(df['UnitPrice'])\n",
    "    log_skew = stats.skew(df['UnitPrice_Log'])\n",
    "    \n",
    "    # Winsorización (outliers extremos)\n",
    "    p99 = df['UnitPrice'].quantile(0.99)\n",
    "    p1 = df['UnitPrice'].quantile(0.01)\n",
    "    df['UnitPrice_Winsorized'] = df['UnitPrice'].clip(lower=p1, upper=p99)\n",
    "    winsor_skew = stats.skew(df['UnitPrice_Winsorized'])\n",
    "    \n",
    "    # Crear variable de valor total\n",
    "    df['TotalValue'] = df['Quantity'] * df['UnitPrice']\n",
    "    df['TotalValue_Log'] = np.log1p(df['TotalValue'].clip(lower=0))\n",
    "    \n",
    "    # Segmentos de precios\n",
    "    df['Price_Segment'] = pd.cut(df['UnitPrice'], \n",
    "                                bins=[0, 1, 5, 20, np.inf], \n",
    "                                labels=['Bajo', 'Medio', 'Alto', 'Premium'])\n",
    "    \n",
    "    print(f\"Transformaciones aplicadas:\")\n",
    "    print(f\"  Sesgo log-transformado: {log_skew:.3f}\")\n",
    "    print(f\"  Sesgo winsorizado: {winsor_skew:.3f}\")\n",
    "    print(f\"  Mejora en sesgo (log): {((original_skew - log_skew) / original_skew * 100):.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def validate_data_quality(df):\n",
    "    \"\"\"\n",
    "    Valida la calidad del dataset procesado\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame procesado\n",
    "        \n",
    "    Returns:\n",
    "        dict: Métricas de calidad\n",
    "    \"\"\"\n",
    "    print(\"=== VALIDACIÓN DE CALIDAD DE DATOS ===\")\n",
    "    \n",
    "    # Completitud\n",
    "    print(f\"Filas: {df.shape[0]:,}\")\n",
    "    print(f\"Columnas: {df.shape[1]}\")\n",
    "    print(f\"Sin duplicados: {df.duplicated().sum() == 0}\")\n",
    "    print(f\"Sin CustomerID nulos: {df['CustomerID'].isnull().sum() == 0}\")\n",
    "    \n",
    "    # Consistencia\n",
    "    print(f\"Quantity > 0: {(df['Quantity'] > 0).all()}\")\n",
    "    print(f\"UnitPrice > 0: {(df['UnitPrice'] > 0).all()}\")\n",
    "    print(f\"Sin transacciones canceladas: {not df['InvoiceNo'].str.startswith('C', na=False).any()}\")\n",
    "    \n",
    "    # Integridad temporal\n",
    "    if 'InvoiceDate' in df.columns:\n",
    "        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "        date_range = (df['InvoiceDate'].max() - df['InvoiceDate'].min()).days\n",
    "        print(f\"Rango temporal: {df['InvoiceDate'].min()} a {df['InvoiceDate'].max()}\")\n",
    "        print(f\"Días de datos: {date_range}\")\n",
    "    \n",
    "    # Integridad de clientes\n",
    "    customer_stats = df.groupby('CustomerID').agg({\n",
    "        'InvoiceNo': 'nunique',\n",
    "        'TotalValue': 'sum'\n",
    "    }).describe()\n",
    "    \n",
    "    print(f\"Clientes únicos: {df['CustomerID'].nunique():,}\")\n",
    "    print(f\"Transacciones por cliente (promedio): {customer_stats['InvoiceNo']['mean']:.1f}\")\n",
    "    print(f\"Valor promedio por cliente: ${customer_stats['TotalValue']['mean']:.2f}\")\n",
    "    \n",
    "    # Distribución geográfica\n",
    "    if 'Country_Tier' in df.columns and 'Region' in df.columns:\n",
    "        print(f\"\\nDistribución por tiers:\")\n",
    "        tier_dist = df['Country_Tier'].value_counts()\n",
    "        for tier, count in tier_dist.items():\n",
    "            print(f\"  {tier}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nDistribución por regiones:\")\n",
    "        region_dist = df['Region'].value_counts()\n",
    "        for region, count in region_dist.items():\n",
    "            print(f\"  {region}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'total_rows': df.shape[0],\n",
    "        'total_columns': df.shape[1],\n",
    "        'unique_customers': df['CustomerID'].nunique(),\n",
    "        'date_range_days': date_range if 'InvoiceDate' in df.columns else None,\n",
    "        'data_quality_score': 100  # Simplificado, se puede expandir\n",
    "    }\n",
    "\n",
    "def preprocess_retail_data(file_path, save_path=None):\n",
    "    \"\"\"\n",
    "    Pipeline completo de preprocesamiento de datos retail\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Ruta al archivo de datos original\n",
    "        save_path (str, optional): Ruta para guardar datos procesados\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset procesado\n",
    "    \"\"\"\n",
    "    print(\"=== INICIANDO PIPELINE DE PREPROCESAMIENTO ===\\n\")\n",
    "    \n",
    "    # 1. Cargar datos\n",
    "    print(\"PASO 1: Cargando datos...\")\n",
    "    df = load_data(file_path)\n",
    "    print()\n",
    "    \n",
    "    # 2. Remover duplicados\n",
    "    print(\"PASO 2: Removiendo duplicados...\")\n",
    "    df = remove_duplicates(df)\n",
    "    print()\n",
    "    \n",
    "    # 3. Manejar CustomerID nulos\n",
    "    print(\"PASO 3: Manejando CustomerID nulos...\")\n",
    "    df = handle_null_customer_id(df)\n",
    "    print()\n",
    "    \n",
    "    # 4. Filtrar cancelaciones\n",
    "    print(\"PASO 4: Filtrando transacciones canceladas...\")\n",
    "    df = filter_cancelled_transactions(df)\n",
    "    print()\n",
    "    \n",
    "    # 5. Manejar datos inconsistentes\n",
    "    print(\"PASO 5: Manejando datos inconsistentes...\")\n",
    "    df = handle_inconsistent_data(df)\n",
    "    print()\n",
    "    \n",
    "    # 6. Balancear datos geográficos\n",
    "    print(\"PASO 6: Balanceando datos geográficos...\")\n",
    "    df = balance_geographical_data(df)\n",
    "    print()\n",
    "    \n",
    "    # 7. Manejar datos sesgados\n",
    "    print(\"PASO 7: Manejando datos sesgados...\")\n",
    "    df = handle_skewed_data(df)\n",
    "    print()\n",
    "    \n",
    "    # 8. Validar calidad\n",
    "    print(\"PASO 8: Validando calidad de datos...\")\n",
    "    quality_metrics = validate_data_quality(df)\n",
    "    print()\n",
    "    \n",
    "    # 9. Guardar si se especifica ruta\n",
    "    if save_path:\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Datos procesados guardados en: {save_path}\")\n",
    "    \n",
    "    print(\"=== PIPELINE COMPLETADO EXITOSAMENTE ===\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4373d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = preprocess_retail_data(\n",
    "    file_path='../data/transacciones_retail.csv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d004b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb9613",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f88ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rfm_features(df, analysis_date=None):\n",
    "    \"\"\"\n",
    "    Crea características RFM (Recency, Frequency, Monetary) para cada cliente\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con datos transaccionales\n",
    "        analysis_date (str, optional): Fecha de análisis. Si no se proporciona, usa la fecha máxima\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con métricas RFM por cliente\n",
    "    \"\"\"\n",
    "    print(\"=== CREANDO CARACTERÍSTICAS RFM ===\")\n",
    "    \n",
    "    # Asegurar que InvoiceDate sea datetime\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "    \n",
    "    # Definir fecha de análisis\n",
    "    if analysis_date is None:\n",
    "        analysis_date = df['InvoiceDate'].max()\n",
    "    else:\n",
    "        analysis_date = pd.to_datetime(analysis_date)\n",
    "    \n",
    "    print(f\"Fecha de análisis: {analysis_date}\")\n",
    "    \n",
    "    # Calcular métricas RFM por cliente\n",
    "    rfm_data = df.groupby('CustomerID').agg({\n",
    "        'InvoiceDate': lambda x: (analysis_date - x.max()).days,  # Recency\n",
    "        'InvoiceNo': 'nunique',  # Frequency\n",
    "        'TotalValue': 'sum'  # Monetary\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Renombrar columnas\n",
    "    rfm_data.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
    "    \n",
    "    # Crear características adicionales\n",
    "    rfm_data['AvgOrderValue'] = rfm_data['Monetary'] / rfm_data['Frequency']\n",
    "    rfm_data['DaysActive'] = df.groupby('CustomerID')['InvoiceDate'].apply(\n",
    "        lambda x: (x.max() - x.min()).days\n",
    "    ).values\n",
    "    \n",
    "    # Métricas adicionales de comportamiento\n",
    "    customer_behavior = df.groupby('CustomerID').agg({\n",
    "        'Quantity': ['sum', 'mean', 'std'],\n",
    "        'UnitPrice': ['mean', 'std'],\n",
    "        'Country': lambda x: x.mode()[0],  # País más frecuente\n",
    "        'StockCode': 'nunique'  # Variedad de productos\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Aplanar nombres de columnas\n",
    "    customer_behavior.columns = [\n",
    "        'CustomerID', 'TotalQuantity', 'AvgQuantity', 'StdQuantity',\n",
    "        'AvgUnitPrice', 'StdUnitPrice', 'MostFrequentCountry', 'ProductVariety'\n",
    "    ]\n",
    "    \n",
    "    # Combinar con RFM\n",
    "    rfm_features = rfm_data.merge(customer_behavior, on='CustomerID', how='left')\n",
    "    \n",
    "    # Manejar valores nulos en desviaciones estándar\n",
    "    rfm_features['StdQuantity'] = rfm_features['StdQuantity'].fillna(0)\n",
    "    rfm_features['StdUnitPrice'] = rfm_features['StdUnitPrice'].fillna(0)\n",
    "    \n",
    "    print(f\"Características RFM creadas para {len(rfm_features)} clientes\")\n",
    "    \n",
    "    return rfm_features\n",
    "\n",
    "def create_rfm_scores(rfm_features):\n",
    "    \"\"\"\n",
    "    Crea scores RFM usando quintiles\n",
    "    \n",
    "    Args:\n",
    "        rfm_features (pd.DataFrame): DataFrame con características RFM\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con scores RFM añadidos\n",
    "    \"\"\"\n",
    "    print(\"=== CREANDO SCORES RFM ===\")\n",
    "    \n",
    "    rfm_scored = rfm_features.copy()\n",
    "    \n",
    "    # Crear scores usando quintiles (1-5)\n",
    "    # Recency: Score más alto para menor recencia (más reciente = mejor)\n",
    "    rfm_scored['R_Score'] = pd.qcut(rfm_scored['Recency'], 5, labels=[5,4,3,2,1])\n",
    "    \n",
    "    # Frequency: Score más alto para mayor frecuencia\n",
    "    rfm_scored['F_Score'] = pd.qcut(rfm_scored['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "    \n",
    "    # Monetary: Score más alto para mayor valor monetario\n",
    "    rfm_scored['M_Score'] = pd.qcut(rfm_scored['Monetary'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "    \n",
    "    # Crear score RFM combinado\n",
    "    rfm_scored['RFM_Score'] = (\n",
    "        rfm_scored['R_Score'].astype(str) + \n",
    "        rfm_scored['F_Score'].astype(str) + \n",
    "        rfm_scored['M_Score'].astype(str)\n",
    "    )\n",
    "    \n",
    "    # Crear segmentos de clientes\n",
    "    def assign_customer_segment(row):\n",
    "        r, f, m = int(row['R_Score']), int(row['F_Score']), int(row['M_Score'])\n",
    "        \n",
    "        if r >= 4 and f >= 4 and m >= 4:\n",
    "            return 'Champions'\n",
    "        elif r >= 3 and f >= 3 and m >= 3:\n",
    "            return 'Loyal_Customers'\n",
    "        elif r >= 4 and f <= 2:\n",
    "            return 'New_Customers'\n",
    "        elif r >= 3 and f <= 2 and m >= 3:\n",
    "            return 'Potential_Loyalists'\n",
    "        elif r <= 2 and f >= 3 and m >= 3:\n",
    "            return 'At_Risk'\n",
    "        elif r <= 2 and f <= 2 and m >= 3:\n",
    "            return 'Cannot_Lose'\n",
    "        elif r >= 3 and f >= 3 and m <= 2:\n",
    "            return 'Price_Sensitive'\n",
    "        else:\n",
    "            return 'Others'\n",
    "    \n",
    "    rfm_scored['Customer_Segment'] = rfm_scored.apply(assign_customer_segment, axis=1)\n",
    "    \n",
    "    # Estadísticas de segmentos\n",
    "    segment_stats = rfm_scored['Customer_Segment'].value_counts()\n",
    "    print(\"Distribución de segmentos de clientes:\")\n",
    "    for segment, count in segment_stats.items():\n",
    "        print(f\"  {segment}: {count:,} ({count/len(rfm_scored)*100:.1f}%)\")\n",
    "    \n",
    "    return rfm_scored\n",
    "\n",
    "def create_target_variable(df, prediction_days=90, analysis_date=None):\n",
    "    \"\"\"\n",
    "    Crea variable objetivo: si el cliente volverá a comprar en los próximos X días\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con datos transaccionales\n",
    "        prediction_days (int): Días hacia el futuro para la predicción\n",
    "        analysis_date (str, optional): Fecha de corte para el análisis\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con variable objetivo por cliente\n",
    "    \"\"\"\n",
    "    print(f\"=== CREANDO VARIABLE OBJETIVO ({prediction_days} días) ===\")\n",
    "    \n",
    "    # Asegurar que InvoiceDate sea datetime\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "    \n",
    "    # Definir fecha de corte\n",
    "    if analysis_date is None:\n",
    "        # Usar una fecha que permita tener datos futuros para validar\n",
    "        analysis_date = df['InvoiceDate'].quantile(0.7)  # 70% de los datos para entrenamiento\n",
    "    else:\n",
    "        analysis_date = pd.to_datetime(analysis_date)\n",
    "    \n",
    "    print(f\"Fecha de corte para análisis: {analysis_date}\")\n",
    "    print(f\"Fecha límite para predicción: {analysis_date + pd.Timedelta(days=prediction_days)}\")\n",
    "    \n",
    "    # Separar datos históricos y futuros\n",
    "    historical_data = df[df['InvoiceDate'] <= analysis_date]\n",
    "    future_data = df[df['InvoiceDate'] > analysis_date]\n",
    "    \n",
    "    print(f\"Registros históricos: {len(historical_data):,}\")\n",
    "    print(f\"Registros futuros: {len(future_data):,}\")\n",
    "    \n",
    "    # Clientes que aparecen en datos históricos\n",
    "    historical_customers = set(historical_data['CustomerID'].unique())\n",
    "    \n",
    "    # Clientes que compran en el período de predicción\n",
    "    future_cutoff = analysis_date + pd.Timedelta(days=prediction_days)\n",
    "    prediction_period_data = future_data[\n",
    "        (future_data['InvoiceDate'] > analysis_date) & \n",
    "        (future_data['InvoiceDate'] <= future_cutoff)\n",
    "    ]\n",
    "    \n",
    "    future_customers = set(prediction_period_data['CustomerID'].unique())\n",
    "    \n",
    "    print(f\"Clientes en período histórico: {len(historical_customers):,}\")\n",
    "    print(f\"Clientes que compran en período de predicción: {len(future_customers):,}\")\n",
    "    \n",
    "    # Crear variable objetivo\n",
    "    target_data = []\n",
    "    \n",
    "    for customer_id in historical_customers:\n",
    "        will_purchase = 1 if customer_id in future_customers else 0\n",
    "        target_data.append({\n",
    "            'CustomerID': customer_id,\n",
    "            'WillPurchase_90Days': will_purchase\n",
    "        })\n",
    "    \n",
    "    target_df = pd.DataFrame(target_data)\n",
    "    \n",
    "    # Estadísticas de la variable objetivo\n",
    "    target_distribution = target_df['WillPurchase_90Days'].value_counts()\n",
    "    print(f\"\\nDistribución de variable objetivo:\")\n",
    "    print(f\"  No volverá a comprar (0): {target_distribution[0]:,} ({target_distribution[0]/len(target_df)*100:.1f}%)\")\n",
    "    print(f\"  Volverá a comprar (1): {target_distribution[1]:,} ({target_distribution[1]/len(target_df)*100:.1f}%)\")\n",
    "    \n",
    "    return target_df, analysis_date\n",
    "\n",
    "def create_temporal_features(df, analysis_date):\n",
    "    \"\"\"\n",
    "    Crea características temporales adicionales\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con datos transaccionales\n",
    "        analysis_date (datetime): Fecha de corte para el análisis\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con características temporales por cliente\n",
    "    \"\"\"\n",
    "    print(\"=== CREANDO CARACTERÍSTICAS TEMPORALES ===\")\n",
    "    \n",
    "    # Filtrar solo datos históricos\n",
    "    historical_data = df[df['InvoiceDate'] <= analysis_date].copy()\n",
    "    \n",
    "    # Crear características temporales\n",
    "    historical_data['Year'] = historical_data['InvoiceDate'].dt.year\n",
    "    historical_data['Month'] = historical_data['InvoiceDate'].dt.month\n",
    "    historical_data['DayOfWeek'] = historical_data['InvoiceDate'].dt.dayofweek\n",
    "    historical_data['Quarter'] = historical_data['InvoiceDate'].dt.quarter\n",
    "    \n",
    "    # Características de comportamiento temporal por cliente\n",
    "    temporal_features = historical_data.groupby('CustomerID').agg({\n",
    "        'InvoiceDate': [\n",
    "            lambda x: x.nunique(),  # Días únicos de compra\n",
    "            lambda x: (analysis_date - x.min()).days,  # Antigüedad del cliente\n",
    "            lambda x: (x.max() - x.min()).days if len(x) > 1 else 0  # Período de actividad\n",
    "        ],\n",
    "        'Year': lambda x: x.nunique(),  # Años únicos de compra\n",
    "        'Month': lambda x: x.nunique(),  # Meses únicos de compra\n",
    "        'DayOfWeek': lambda x: x.mode()[0] if len(x) > 0 else 0,  # Día preferido\n",
    "        'Quarter': lambda x: x.nunique()  # Trimestres únicos\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Aplanar nombres de columnas\n",
    "    temporal_features.columns = [\n",
    "        'CustomerID', 'UniquePurchaseDays', 'CustomerAge_Days', 'ActivityPeriod_Days',\n",
    "        'UniqueYears', 'UniqueMonths', 'PreferredDayOfWeek', 'UniqueQuarters'\n",
    "    ]\n",
    "    \n",
    "    # Calcular frecuencia de compra promedio\n",
    "    temporal_features['AvgDaysBetweenPurchases'] = (\n",
    "        temporal_features['ActivityPeriod_Days'] / \n",
    "        (temporal_features['UniquePurchaseDays'] - 1)\n",
    "    ).fillna(0)\n",
    "    \n",
    "    print(f\"Características temporales creadas para {len(temporal_features)} clientes\")\n",
    "    \n",
    "    return temporal_features\n",
    "\n",
    "def combine_all_features(rfm_features, target_df, temporal_features):\n",
    "    \"\"\"\n",
    "    Combina todas las características en un dataset final\n",
    "    \n",
    "    Args:\n",
    "        rfm_features (pd.DataFrame): Características RFM con scores\n",
    "        target_df (pd.DataFrame): Variable objetivo\n",
    "        temporal_features (pd.DataFrame): Características temporales\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset final con todas las características\n",
    "    \"\"\"\n",
    "    print(\"=== COMBINANDO TODAS LAS CARACTERÍSTICAS ===\")\n",
    "    \n",
    "    # Combinar todas las características\n",
    "    final_dataset = rfm_features.merge(target_df, on='CustomerID', how='inner')\n",
    "    final_dataset = final_dataset.merge(temporal_features, on='CustomerID', how='left')\n",
    "    \n",
    "    # Manejar valores nulos en características temporales\n",
    "    temporal_cols = ['UniquePurchaseDays', 'CustomerAge_Days', 'ActivityPeriod_Days',\n",
    "                    'UniqueYears', 'UniqueMonths', 'PreferredDayOfWeek', 'UniqueQuarters',\n",
    "                    'AvgDaysBetweenPurchases']\n",
    "    \n",
    "    for col in temporal_cols:\n",
    "        if col in final_dataset.columns:\n",
    "            final_dataset[col] = final_dataset[col].fillna(0)\n",
    "    \n",
    "    # Convertir scores RFM a numérico\n",
    "    final_dataset['R_Score'] = final_dataset['R_Score'].astype(int)\n",
    "    final_dataset['F_Score'] = final_dataset['F_Score'].astype(int)\n",
    "    final_dataset['M_Score'] = final_dataset['M_Score'].astype(int)\n",
    "    \n",
    "    print(f\"Dataset final creado:\")\n",
    "    print(f\"  Filas: {len(final_dataset):,}\")\n",
    "    print(f\"  Columnas: {final_dataset.shape[1]}\")\n",
    "    print(f\"  Clientes únicos: {final_dataset['CustomerID'].nunique():,}\")\n",
    "    \n",
    "    return final_dataset\n",
    "\n",
    "def feature_engineering_pipeline(df, prediction_days=90, save_path=None):\n",
    "    \"\"\"\n",
    "    Pipeline completo de feature engineering\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame procesado con datos transaccionales\n",
    "        prediction_days (int): Días para predicción futura\n",
    "        save_path (str, optional): Ruta para guardar el dataset final\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset final con todas las características\n",
    "    \"\"\"\n",
    "    print(\"=== INICIANDO PIPELINE DE FEATURE ENGINEERING ===\\n\")\n",
    "    \n",
    "    # 1. Crear características RFM\n",
    "    print(\"PASO 1: Creando características RFM...\")\n",
    "    rfm_features = create_rfm_features(df)\n",
    "    print()\n",
    "    \n",
    "    # 2. Crear scores RFM\n",
    "    print(\"PASO 2: Creando scores RFM...\")\n",
    "    rfm_features = create_rfm_scores(rfm_features)\n",
    "    print()\n",
    "    \n",
    "    # 3. Crear variable objetivo\n",
    "    print(\"PASO 3: Creando variable objetivo...\")\n",
    "    target_df, analysis_date = create_target_variable(df, prediction_days)\n",
    "    print()\n",
    "    \n",
    "    # 4. Crear características temporales\n",
    "    print(\"PASO 4: Creando características temporales...\")\n",
    "    temporal_features = create_temporal_features(df, analysis_date)\n",
    "    print()\n",
    "    \n",
    "    # 5. Combinar todas las características\n",
    "    print(\"PASO 5: Combinando características...\")\n",
    "    final_dataset = combine_all_features(rfm_features, target_df, temporal_features)\n",
    "    print()\n",
    "    \n",
    "    # 6. Guardar si se especifica ruta\n",
    "    if save_path:\n",
    "        final_dataset.to_csv(save_path, index=False)\n",
    "        print(f\"Dataset con feature engineering guardado en: {save_path}\")\n",
    "    \n",
    "    print(\"=== PIPELINE DE FEATURE ENGINEERING COMPLETADO ===\")\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = feature_engineering_pipeline(\n",
    "    df_processed,\n",
    "    prediction_days=90,\n",
    "    save_path='../data/retail_features_dataset.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb6e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDataset final con feature engineering:\")\n",
    "print(f\"Forma: {df_features.shape}\")\n",
    "print(f\"\\nColumnas disponibles:\")\n",
    "for i, col in enumerate(df_features.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4519ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nPrimeras 5 filas:\")\n",
    "df_features.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
